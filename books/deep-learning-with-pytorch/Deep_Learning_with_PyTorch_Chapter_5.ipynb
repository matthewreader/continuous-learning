{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with PyTorch - Chapter 5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6zh5Dhc1SrK2XqJtohNYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewreader/continuous-learning/blob/main/books/deep-learning-with-pytorch/Deep_Learning_with_PyTorch_Chapter_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Learning as Parameter Estimation** \n",
        "\n",
        "We'll take some temperatures in celsius and their corresponding temperature in an unknown unit.  This simple regression problem shows how with enough data, we can estimate what the unknown unit temperature should be given a temperature in celsius."
      ],
      "metadata": {
        "id": "C_7A-FA48ru8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOx5Yc68Wnc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(t_u, t_c)\n",
        "plt.xlabel(\"Unknown\")\n",
        "plt.ylabel(\"Celsius\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "P4HlzLEy8cxe",
        "outputId": "3aa13d21-faa9-4ec5-c173-635a7892f1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAU4klEQVR4nO3df5BdZ33f8fcHWUwWY7p2rHHtNYkMoco4OFh060JxPPyMwPFg4Wb4MUDdhMR0Bqa4AwqITgtJJ2NSOSZkYJjYwY2dAgkJsmEoiSDg1tAkJCvkIGNH5ZcNXv9aCopNskNs8e0f9yxabSXrrr333r33eb9mNHvvc+6P7zPaez57n/Oc56SqkCS153GjLkCSNBoGgCQ1ygCQpEYZAJLUKANAkhp1wqgLWI1TTz21Nm/ePOoyJGms7N2799tVtWll+1gFwObNm5mbmxt1GZI0VpLcebR2h4AkqVEGgCQ1ygCQpEYZAJLUKANAkho1VrOAJKk1N+6bZ9eeA9x9cJEzpqfYsW0L27fOrMlrGwCStE7duG+enbv3s/jQIQDmDy6yc/d+gDUJAYeAJGmd2rXnwA93/ksWHzrErj0H1uT1DQBJWqfuPri4qvbVMgAkaZ06Y3pqVe2rZQBI0jq1Y9sWpjZuOKJtauMGdmzbsiav70FgSVqnlg70OgtIkhq0fevMmu3wV3IISJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJatTAAyDJk5PclOS2JF9O8qau/Z1J5pPc0v27cNC1SJIOG8ZicA8Db66qLyY5Cdib5NPdtndX1ZVDqEGStMLAA6Cq7gHu6W4/mOR2YDBL20mS+jbUYwBJNgNbgS90TW9M8qUk1yY5+RjPuSzJXJK5hYWFIVUqSZNvaAGQ5InAR4HLq+oB4P3AU4Fz6X1D+M2jPa+qrq6q2aqa3bRp07DKlaSJN5QASLKR3s7/g1W1G6Cq7quqQ1X1A+Aa4Lxh1CJJ6hnGLKAAHwBur6qrlrWfvuxhLwNuHXQtkqTDhjEL6DnAa4H9SW7p2t4OvCrJuUABdwCvH0ItkqTOMGYBfR7IUTZ9ctDvLUk6Ns8ElqRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVHDuCCMJPXlxn3z7NpzgLsPLnLG9BQ7tm1h+9aZUZc1sQwASevCjfvm2bl7P4sPHQJg/uAiO3fvBzAEBsQhIEnrwq49B36481+y+NAhdu05MKKKJp8BIGlduPvg4qra9dgZAJLWhTOmp1bVrsfOAJC0LuzYtoWpjRuOaJvauIEd27aMqKLJ50FgSevC0oFeZwENz8ADIMmTgeuB04ACrq6q9yQ5BfhDYDNwB/DyqvruoOuRtH5t3zrjDn+IhjEE9DDw5qo6G3gW8IYkZwNvAz5TVU8DPtPdlyQNycADoKruqaovdrcfBG4HZoCLgeu6h10HbB90LZKkw4Z6EDjJZmAr8AXgtKq6p9t0L70hoqM957Ikc0nmFhYWhlKnJLVgaAGQ5InAR4HLq+qB5duqqugdH/j/VNXVVTVbVbObNm0aQqWS1IahBECSjfR2/h+sqt1d831JTu+2nw7cP4xaJEk9Aw+AJAE+ANxeVVct2/Rx4NLu9qXAxwZdiyTpsGGcB/Ac4LXA/iS3dG1vB94FfCTJ64A7gZcPoRZJUmfgAVBVnwdyjM0vGPT7S5KOzqUgJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkho18ABIcm2S+5PcuqztnUnmk9zS/btw0HVIko50whDe4/eA9wLXr2h/d1VdOYT3lybajfvm2bXnAHcfXOSM6Sl2bNvC9q0zoy5LY2DgAVBVNyfZPOj3kVp04755du7ez+JDhwCYP7jIzt37AQwBHdcojwG8McmXuiGik0dYhzS2du058MOd/5LFhw6xa8+BEVWkcTKqAHg/8FTgXOAe4DeP9cAklyWZSzK3sLAwrPqksXD3wcVVtUvLjSQAquq+qjpUVT8ArgHOe4THXl1Vs1U1u2nTpuEVKY2BM6anVtUuLTeSAEhy+rK7LwNuPdZjJR3bjm1bmNq44Yi2qY0b2LFty4gq0jhZ9UHgJI8DnlhVD/T5+A8DzwVOTXIX8A7guUnOBQq4A3j9auuQJtVqZvUstTsLSI9Gqur4D0o+BPw74BDw18CTgPdU1a7Blnek2dnZmpubG+ZbSkO1clYP9P6iv+KSc9yp61FLsreqZle29zsEdHb3F/924E+As4DXrmF9knBWj4ar3wDYmGQjvQD4eFU9RG/4RtIaclaPhqnfAPgdemP1JwI3J/lxoK9jAJL656weDVNfAVBVv11VM1V1YfXcCTxvwLVJzXFWj4apr1lASf7zMTb92hrWIjVv0LN6XDdIy/U7DfTvl93+EeAi4Pa1L0fS9q0zA9kpu26QVuorAKrqiKUaklwJ7BlIRZIG4pFmGBkAbXq0ZwI/AThzLQuRNFjOMNJK/R4D2M/haZ8bgE04/i+NlTOmp5g/ys7eGUbt6vcYwEXLbj8M3FdVDw+gHkkDsmPblqOeZewMo3Y9YgAkeVJ3BvCDKzY9KQlV9Z3BlSZpLblukFY63jeAD9H7638vvSGgLNtWwFMGVJekARjUDCONp0cMgKq6qPt51nDKkSQNS1+zgJI8J8mJ3e3XJLkqyY8NtjRJ0iD1Ow30/cA/JHkG8Gbga8DvD6wqSdLA9RsAD1fvwgEXA++tqvcBJw2uLEnSoPU7DfTBJDuB1wAXdFcF2zi4siS5bo8Grd9vAK8Avg+8rqrupXcW8FCvBia1ZGndnvmDixSH1+25cd/8qEvTBOl3Oeh7q+qqqvpcd/+bVXX9YEuT2uWVwTQMxzsR7EGOfuWvAFVVTxpIVVLjXLdHw3C88wA80CuNgOv2aBj6Xg00yflJfqG7fWoSTw6TBsQrg2kY+j0R7B3AW4GdXdPjgf/e53OvTXJ/kluXtZ2S5NNJvtL9PHm1hUuTbPvWGa645BxmpqcIMDM9xRWXnOMsIK2p9Kb3H+dByS3AVuCLVbW1a/tSVf10H8+9APgecH1VPb1r+6/Ad6rqXUneBpxcVW893mvNzs7W3NzcceuVJB2WZG9Vza5s73cI6B+7E8Gqe7ET+33jqroZWLlq6MXAdd3t64Dt/b6eJGlt9BsAH0nyO8B0kl8G/gy45jG872lVdU93+17gtGM9MMllSeaSzC0sLDyGt5QkLXe8aaA/QW9nfWWSFwEPAFuAPwE+uRYFVFUlOeY4VFVdDVwNvSGgtXhPSdLxvwH8Fr2dPlX16araUVVvAW7otj1a9yU5HaD7ef9jeC1J0qNwvAA4rar2r2zs2jY/hvf9OHBpd/tS4GOP4bUkSY/C8QJg+hG29XVGSpIPA38BbElyV5LXAe8CXpTkK8ALu/uSpCE63mqgc0l+uaqOOOCb5JfoXSbyuKrqVcfY9IJ+ni9JGozjBcDlwA1JXs3hHf4svRPBXjbIwiRJg3W8tYDuA/5VkucBT++a/0dVfXbglUmSBqqvC8JU1U3ATQOuRZI0RH0vBidJmiz9XhJSmlheelGtMgDUtKVLLy5dfWvp0ouAIaCJ5xCQmualF9UyA0BN89KLapkBoKYd6xKLXnpRLTAA1DQvvaiWeRBYTVs60OssILXIAFDztm+dcYevJjkEJEmNMgAkqVEGgCQ1ygCQpEZ5EFjNcg0gtc4AUJNcA0hyCEiNcg0gyQBQo1wDSDIA1CjXAJIMADXKNYCkER8ETnIH8CBwCHi4qmZHWY/a4RpA0vqYBfS8qvr2qItQe1wDSK1zCEiSGjXqACjgU0n2JrnsaA9IclmSuSRzCwsLQy5PkibXqAPg/Kp6JvAS4A1JLlj5gKq6uqpmq2p206ZNw69QkibUSAOgqua7n/cDNwDnjbIeSWrJyAIgyYlJTlq6DfwscOuo6pGk1oxyFtBpwA1Jlur4UFX96QjrkaSmjCwAqurrwDNG9f6S1LpRHwSWJI2IASBJjTIAJKlRBoAkNWo9rAWkNealDiX1wwCYMF7qUFK/HAKaMF7qUFK/DIAJ46UOJfXLAJgwXupQUr8MgAnjpQ4l9cuDwBPGSx1K6pcBMIG81KGkfjgEJEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSozwPoGEuGy21zQBolMtGS3IIqFEuGy1ppAGQ5MVJDiT5apK3jbKW1rhstKSRBUCSDcD7gJcAZwOvSnL2qOppjctGSxrlN4DzgK9W1der6h+BPwAuHmE9TXHZaEmjDIAZ4FvL7t/VtR0hyWVJ5pLMLSwsDK24Sbd96wxXXHIOM9NTBJiZnuKKS87xALDUkHU/C6iqrgauBpidna0RlzNRXDZaatsovwHMA09edv/Mrk2SNASjDIC/Bp6W5KwkjwdeCXx8hPVIUlNGNgRUVQ8neSOwB9gAXFtVXx5VPZLUmpEeA6iqTwKfHGUNktQqzwSWpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDVq3V8U/rG6cd88u/Yc4O6Di5wxPcWObVu8ELokMeEBcOO+eXbu3s/iQ4cAmD+4yM7d+wEMAUnNm+ghoF17Dvxw579k8aFD7NpzYEQVSdL6MdEBcPfBxVW1S1JLJjoAzpieWlW7JLVkogNgx7YtTG3ccETb1MYN7Ni2ZUQVSdL6MZIASPLOJPNJbun+XTiI99m+dYYrLjmHmekpAsxMT3HFJed4AFiSGO0soHdX1ZWDfpPtW2fc4UvSUUz0EJAk6dhGGQBvTPKlJNcmOflYD0pyWZK5JHMLCwvDrE+SJlqqajAvnPwZ8E+Psuk/An8JfBso4L8Ap1fVLx7vNWdnZ2tubm5N65SkSZdkb1XNrmwf2DGAqnphP49Lcg3wiUHVIUk6ulHNAjp92d2XAbeOog5JatnAhoAe8U2T3wfOpTcEdAfw+qq6p4/nLQB3HmXTqfSGlCbBpPRlUvoB9mU9mpR+wHD68uNVtWll40gCYK0lmTva+NY4mpS+TEo/wL6sR5PSDxhtX5wGKkmNMgAkqVGTEgBXj7qANTQpfZmUfoB9WY8mpR8wwr5MxDEASdLqTco3AEnSKhkAktSosQqAJE9OclOS25J8OcmbuvZTknw6yVe6n8dcW2i9SPIjSf4qyd90ffnVrv2sJF9I8tUkf5jk8aOutV9JNiTZl+QT3f2x60uSO5Ls75Ypn+vaxu73CyDJdJI/TvK3SW5P8uxx7EuSLcuWjr8lyQNJLh/TvvyH7vN+a5IPd/uBkX1OxioAgIeBN1fV2cCzgDckORt4G/CZqnoa8Jnu/nr3feD5VfUMeifFvTjJs4DfoLdU9k8A3wVeN8IaV+tNwO3L7o9rX55XVecum5s9jr9fAO8B/rSqfhJ4Br3/m7HrS1Ud6P4/zgX+OfAPwA2MWV+SzAD/HpitqqcDG4BXMsrPSVWN7T/gY8CLgAP0FpQDOB04MOraVtmPJwBfBP4lvTMCT+janw3sGXV9ffbhTHofwufTW9sp49gXememn7qibex+v4B/AnyDbqLHOPdlRf0/C/zvcewLMAN8CziF3jpsnwC2jfJzMm7fAH4oyWZgK/AF4LQ6vJTEvcBpIyprVbohk1uA+4FPA18DDlbVw91D7qL3SzMOfgv4FeAH3f0fZTz7UsCnkuxNclnXNo6/X2cBC8B/64blfjfJiYxnX5Z7JfDh7vZY9aWq5oErgW8C9wB/B+xlhJ+TsQyAJE8EPgpcXlUPLN9WvRgdi7mtVXWoel9rzwTOA35yxCU9KkkuAu6vqr2jrmUNnF9VzwReQm+I8YLlG8fo9+sE4JnA+6tqK/D3rBgiGaO+ANCNjb8U+KOV28ahL90xiovphfMZwInAi0dZ09gFQJKN9Hb+H6yq3V3zfUsrjHY/7x9VfY9GVR0EbqL39W86ydIy3WcC8yMrrH/PAV6a5A7gD+gNA72HMexL91caVXU/vXHm8xjP36+7gLuq6gvd/T+mFwjj2JclLwG+WFX3dffHrS8vBL5RVQtV9RCwm95nZ2Sfk7EKgCQBPgDcXlVXLdv0ceDS7val9I4NrGtJNiWZ7m5P0TuWcTu9IPj57mFj0Zeq2llVZ1bVZnpf0T9bVa9mzPqS5MQkJy3dpjfefCtj+PtVVfcC30qypWt6AXAbY9iXZV7F4eEfGL++fBN4VpIndPuypf+TkX1OxupM4CTnA58D9nN4rPnt9I4DfAT4MXrLRb+8qr4zkiL7lOSngevozQR4HPCRqvq1JE+h91f0KcA+4DVV9f3RVbo6SZ4LvKWqLhq3vnT13tDdPQH4UFX9epIfZcx+vwCSnAv8LvB44OvAL9D9rjF+fTmR3g70KVX1d13b2P2/dNO9X0FvRuM+4JfojfmP5HMyVgEgSVo7YzUEJElaOwaAJDXKAJCkRhkAktQoA0CSGmUAqClJNie5dUXbO5O85RGe82+TvHfw1UnDZQBIUqMMAKmT5H8m+Y3uOg3/J8nPHOUxP5fkL5KcmuT3kvx2kj9P8vUkP989Jkl2dWu+70/yiq79fUle2t2+Icm13e1fTPLr3beT25Nc060Z/6nuLHFpIAwA6UgnVNV5wOXAO5ZvSPIyeguqXVhV3+6aTwfOBy4C3tW1XULvGg/PoLf+y65urZrPAUuhMgOc3d3+GeDm7vbTgPdV1U8BB4F/vaa9k5YxANSaY536vtS+tMDgXmDzsu3PB94K/FxVfXdZ+41V9YOquo3DyxGfD3y4W+31PuB/Af+CLgC6ixjdxuHFzJ4N/Hn33G9U1S3HqEFaUwaAWvN/gZWXDjyF3kU5oHelNoBD9NYDWvI14CTgn6147vI1W/JIb9ytNDpNbwngm+kFwsuB71XVg0d5vZU1SGvKAFBTqup7wD1Jng+96/3S2yF//jhPvZPecMz1SX7qOI/9HPCK7oI/m4ALgL/qtv0lveGlpQB4S/dTGjoDQC36N8B/6q7G9lngV6vqa8d7UlX9LfBq4I+SPPURHnoD8CXgb7rX/5VueWbo7exPqKqv0rsM6CkYABoRVwOVpEb5DUCSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEb9P3bI/GJNSu+fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u, w, b):\n",
        "   return w * t_u + b\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "   squared_diffs = (t_p - t_c)**2\n",
        "   return squared_diffs.mean()  \n",
        "\n",
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "t_p = model(t_u, w, b)\n",
        "t_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biD9kkqD9UsK",
        "outputId": "0baab158-5bb9-4624-d523-302015f84080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
              "        48.4000, 60.4000, 68.4000])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(t_p, t_c)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvrirdnrAEJZ",
        "outputId": "c931b9ed-48f6-42c3-db79-0bb2760a47dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1763.8848)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our above model is pretty useless at this point:\n",
        "\n",
        "Celsius = (1 * Unknown) + 0\n",
        "\n",
        "Or Celsius = Unknown, which we know is not true.  We can tweak the weights and bias of our model using PyTorch to create a useful model.  As we tweak out weights, we know we're getting closer to a useful model when we minimize our loss function."
      ],
      "metadata": {
        "id": "_UW3dUXkApba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta = 0.1\n",
        "\n",
        "loss_rate_of_change_w = \\\n",
        "   (loss_fn(model(t_u, w + delta, b), t_c) -\n",
        "    loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
      ],
      "metadata": {
        "id": "cDrnTTf-ASt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rate of change in our loss function as we update `w` and `b` might not be uniform as we minimize the loss function.  Therefore we typically make a very small change each time we update these parameters.  This is referred to as the learning rate."
      ],
      "metadata": {
        "id": "fYjxsnespv9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "\n",
        "w = w - learning_rate * loss_rate_of_change_w\n",
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4yoIvNKlhz6",
        "outputId": "f530fc34-8331-4c88-fde4-532286b3d781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-44.1730)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_rate_of_change_b = \\\n",
        "   (loss_fn(model(t_u, w, b + delta), t_c) -\n",
        "    loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
        "\n",
        "b = b - learning_rate * loss_rate_of_change_b\n",
        "b\n",
        "\n",
        "# Normalize input\n",
        "t_un = 0.1 * t_u"
      ],
      "metadata": {
        "id": "aYqVO0h6qSru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rates are important because if the learning rate is too large, the updates to parameters could be too large.  This leads to the loss function diverging instead of converging to the minimum.\n",
        "\n",
        "We may also need to normalize the inputs.  Consider that if the gradient for a weight `w` is greater than the gradient for bias `b`, then they are in differently scaled spaces."
      ],
      "metadata": {
        "id": "9iM7cAiBuNfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.5 - PyTorch autograd and backpropagation**"
      ],
      "metadata": {
        "id": "35K3cTWj3sHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall the following for our model and loss functions:\n",
        "def model(t_u, w, b):\n",
        "   return w * t_u + b\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "   squared_diffs = (t_p - t_c)**2\n",
        "   return squared_diffs.mean()\n",
        "\n",
        "# Initialize our parameters with requires_grad=True\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)"
      ],
      "metadata": {
        "id": "7-CxAL4WqWI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(model(t_u, *params), t_c)\n",
        "loss.backward()\n",
        "\n",
        "params.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f1G1iPU3qY9",
        "outputId": "27982696-25f8-4bc8-f13e-23a3128f875b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4517.2969,   82.6000])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling `backward` will lead derivatives to accumulate at leaf nodes.  Gradients should be zeroed explicitly after using it for parameter updates."
      ],
      "metadata": {
        "id": "j7JZ8vSi5Ep7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if params.grad is not None:\n",
        "   params.grad.zero_()"
      ],
      "metadata": {
        "id": "1GlOwOa64Zg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        if params.grad is not None:\n",
        "            params.grad.zero_()\n",
        " \n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "        loss.backward()\n",
        " \n",
        "        with torch.no_grad():\n",
        "            params -= learning_rate * params.grad\n",
        " \n",
        "        if epoch % 500 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        " \n",
        "    return params"
      ],
      "metadata": {
        "id": "i7rA2pr35jPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    learning_rate = 1e-2,\n",
        "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVIlupG85j0S",
        "outputId": "2f6965a1-a849-49eb-eeec-d3574127e037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.860115\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957698\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could use other optimizers other than gradient descent.  Gradient descent works well enough for our simple problem, but another optimizer might work better for more complicated problems."
      ],
      "metadata": {
        "id": "HhsM3sKZ6b-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "dir(optim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWsqcesT6FFy",
        "outputId": "f927b3d6-6603-486f-b0f3-cf78ecb8cbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASGD',\n",
              " 'Adadelta',\n",
              " 'Adagrad',\n",
              " 'Adam',\n",
              " 'AdamW',\n",
              " 'Adamax',\n",
              " 'LBFGS',\n",
              " 'NAdam',\n",
              " 'Optimizer',\n",
              " 'RAdam',\n",
              " 'RMSprop',\n",
              " 'Rprop',\n",
              " 'SGD',\n",
              " 'SparseAdam',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_functional',\n",
              " '_multi_tensor',\n",
              " 'lr_scheduler',\n",
              " 'swa_utils']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create `params` and instantiate a gradient descent optimizer"
      ],
      "metadata": {
        "id": "afaLbkzIDfqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "t_p = model(t_u, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()\n",
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksl0HxIi6oEa",
        "outputId": "f9ed6562-6ed5-4f14-984b-25c7ca5d17ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using out stochastic gradient descent optimizer, making sure to zero our gradients each epoch."
      ],
      "metadata": {
        "id": "dNZEsTBjE430"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "    return params\n",
        "\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRcwekCHDnOr",
        "outputId": "89d84be0-d2f0-4b8a-b185-a8f32c0f3e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Loss 7.860120\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957698\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jUvcVFqIFHcZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}